{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion of Features\n",
    "\n",
    "In this notebook we will develop the pipeline for the fusion of extracted radiomics and deeply learnt features using the ACDC dataset. \n",
    "\n",
    "We will retake the pipeline used for the Radiomics supervised learning section of the project, but now we will add the deeply learnt features preprocessing and combination with radiomics. \n",
    "\n",
    "We will study predictive performance, feature selection and accuracy metrics either indidividually and combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the packages and libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, precision_score\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, RFECV, RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Radiomics\n",
    "\n",
    "path1= '/home/alejandro/Documentos/files_master'\n",
    "\n",
    "RF_df_train = pd.read_csv(path1+ '/ACDC_(Radiomics+Clinical)_Training.csv')\n",
    "print(RF_df_train.shape)\n",
    "RF_df_test = pd.read_csv(path1+ '/ACDC_(Radiomics+Clinical)_Testing.csv')\n",
    "print(RF_df_test.shape)\n",
    "\n",
    "\n",
    "RF_df_train = RF_df_train.loc[:,~ RF_df_train.columns.str.startswith('diagnostics')]\n",
    "RF_df_test = RF_df_test.loc[:,~ RF_df_test.columns.str.startswith('diagnostics')]\n",
    "print(RF_df_train.shape)\n",
    "print(RF_df_test.shape)\n",
    "\n",
    "\n",
    "# Also, we separate the medical inputs and separate the independent variable (disease)\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "radiomics_train = RF_df_train.iloc[:,:-3]\n",
    "med_info_train = RF_df_train.iloc[:,-3:-1]\n",
    "y_train = RF_df_train.iloc[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "radiomics_test = RF_df_test.iloc[:,:-3]\n",
    "med_info_test = RF_df_test.iloc[:,-3:-1]\n",
    "y_test = RF_df_test.iloc[:,-1]\n",
    "\n",
    "\n",
    "#### Deeply Learned Features\n",
    "### Inception\n",
    "\n",
    "path = '/home/alejandro/Documentos/files_master/ACDC_Fusion/Ask_fus/Short Format'\n",
    "\n",
    "incep_DF_train_ED = pd.read_csv(path+'/DLR_IncepModel_train_ED_2.csv', header=None)\n",
    "incep_DF_train_ES = pd.read_csv(path+'/DLR_IncepModel_train_ES_2.csv', header=None)\n",
    "incep_DF_test_ED = pd.read_csv(path+'/DLR_IncepModel_test_ED_2.csv', header=None)\n",
    "incep_DF_test_ES = pd.read_csv(path+'/DLR_IncepModel_test_ES_2.csv', header=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# AlexNet_DF_train_ED = pd.read_csv(path+'/DLR_AlexNet_train_ED_2.csv', header=None)\n",
    "# AlexNet_DF_train_ES = pd.read_csv(path+'/DLR_AlexNet_train_ES_2.csv', header=None)\n",
    "# AlexNet_DF_test_ED = pd.read_csv(path+'/DLR_AlexNet_test_ED_2.csv', header=None)\n",
    "# AlexNet_DF_test_ES = pd.read_csv(path+'/DLR_AlexNet_test_ES_2.csv', header=None)\n",
    "\n",
    "path2 = '/home/alejandro/Documentos/files_master/ACDC_Fusion/Ask_fus'\n",
    "\n",
    "AlexNet_DF_train_ED = pd.read_csv(path2+'/DLR_AlexNet_train_ED_3.csv', header=None)\n",
    "AlexNet_DF_train_ES = pd.read_csv(path2+'/DLR_AlexNet_train_ES_3.csv', header=None)\n",
    "AlexNet_DF_test_ED = pd.read_csv(path2+'/DLR_AlexNet_test_ED_3.csv', header=None)\n",
    "AlexNet_DF_test_ES = pd.read_csv(path2+'/DLR_AlexNet_test_ES_3.csv', header=None)\n",
    "\n",
    "#Late Merging Model\n",
    "LM_DF_train_ES = pd.read_csv(path+'/DLR_LM_features_train_ES_2.csv', header=None)\n",
    "LM_DF_train_ED = pd.read_csv(path+'/DLR_LM_features_train_ED_2.csv', header=None)\n",
    "LM_DF_test_ES = pd.read_csv(path+'/DLR_LM_features_test_ES_2.csv', header=None)\n",
    "LM_DF_test_ED = pd.read_csv(path+'/DLR_LM_features_test_ED_2.csv', header=None)\n",
    "\n",
    "\n",
    "# In[77]:\n",
    "\n",
    "#\n",
    "# LM_DF_train.shape\n",
    "#\n",
    "#\n",
    "# # In[78]:\n",
    "#\n",
    "#\n",
    "# np_LM_train=np.asanyarray(LM_DF_train)\n",
    "# np_LM_train.shape\n",
    "\n",
    "\n",
    "# In[79]:\n",
    "\n",
    "\n",
    "def divide_LM_train():\n",
    "    ED_list = list()\n",
    "    ES_list = list()\n",
    "    \n",
    "    for i in np.arange(1,600,3):\n",
    "        if (i % 2) == 0:\n",
    "            ES_list.append(np_LM_train[i,:])\n",
    "        else:\n",
    "            ED_list.append(np_LM_train[i,:])\n",
    "         \n",
    "    ES = np.asanyarray(ES_list)\n",
    "    ED = np.asanyarray(ED_list)\n",
    "\n",
    "    return ES, ED \n",
    "\n",
    "\n",
    "# In[80]:\n",
    "\n",
    "\n",
    "# ES, ED = divide_LM_train()\n",
    "\n",
    "\n",
    "# In[81]:\n",
    "\n",
    "\n",
    "# LM_train_ES = pd.DataFrame(ES)\n",
    "# LM_train_ED = pd.DataFrame(ED)\n",
    "\n",
    "\n",
    "# In[82]:\n",
    "\n",
    "\n",
    "# LM_DF_test_ES.shape\n",
    "\n",
    "\n",
    "# In[83]:\n",
    "\n",
    "\n",
    "def col_names(df, cycle):\n",
    "    col_names= ['{}_dlf_{}'.format(cycle, x) for x in range(len(df.columns))]\n",
    "    df.columns = col_names\n",
    "    \n",
    "    return df    \n",
    "\n",
    "\n",
    "# In[105]:\n",
    "\n",
    "\n",
    "#### Creation of datasets\n",
    "incep_DF_train_ED = col_names(incep_DF_train_ED, cycle='ED')\n",
    "incep_DF_train_ES = col_names(incep_DF_train_ES, cycle='ES')\n",
    "\n",
    "incep_DF_train = pd.concat([incep_DF_train_ED, incep_DF_train_ES], axis= 1)\n",
    "incep_train_tot = pd.concat([incep_DF_train, med_info_train], axis=1)\n",
    "\n",
    "incep_DF_test_ED = col_names(incep_DF_test_ED , cycle='ED')\n",
    "incep_DF_test_ES = col_names(incep_DF_test_ES, cycle='ES')\n",
    "\n",
    "incep_DF_test = pd.concat([incep_DF_test_ED, incep_DF_test_ES], axis= 1)\n",
    "incep_test_tot = pd.concat([incep_DF_test, med_info_test], axis=1)\n",
    "\n",
    "\n",
    "# In[104]:\n",
    "\n",
    "\n",
    "AlexNet_DF_train_ED = col_names(AlexNet_DF_train_ED, cycle='ED')\n",
    "AlexNet_DF_train_ES = col_names(AlexNet_DF_train_ES, cycle= 'ES')\n",
    "\n",
    "AlexNet_DF_train = pd.concat([AlexNet_DF_train_ED, AlexNet_DF_train_ES], axis= 1)\n",
    "AlexNet_train_tot = pd.concat([AlexNet_DF_train, med_info_train], axis = 1)\n",
    "\n",
    "AlexNet_DF_test_ED = col_names(AlexNet_DF_test_ED, cycle='ED')\n",
    "AlexNet_DF_test_ES = col_names(AlexNet_DF_test_ES, cycle='ES')\n",
    "\n",
    "AlexNet_DF_test = pd.concat([AlexNet_DF_test_ED, AlexNet_DF_test_ES], axis= 1)\n",
    "AlexNet_test_tot = pd.concat([AlexNet_DF_test, med_info_test], axis = 1)\n",
    "\n",
    "\n",
    "# In[180]:\n",
    "\n",
    "\n",
    "# LM_train_ED = col_names(LM_train_ED, cycle='ED')\n",
    "# LM_train_ES = col_names(LM_train_ES, cycle='ES')\n",
    "#\n",
    "# LM_train = pd.concat([LM_train_ED, LM_train_ES], axis=1)\n",
    "# LM_train_tot = pd.concat([LM_train, med_info_train], axis=1)\n",
    "\n",
    "LM_DF_train_ED = col_names(LM_DF_train_ED, cycle='ED')\n",
    "LM_DF_train_ES = col_names(LM_DF_train_ES, cycle='ES')\n",
    "\n",
    "LM_DF_train = pd.concat([LM_DF_train_ED, LM_DF_train_ES], axis= 1)\n",
    "LM_train_tot = pd.concat([LM_DF_train, med_info_train], axis=1)\n",
    "\n",
    "LM_DF_test_ED = col_names(LM_DF_test_ED, cycle='ED')\n",
    "LM_DF_test_ES = col_names(LM_DF_test_ES, cycle='ES')\n",
    "\n",
    "LM_test = pd.concat([LM_DF_test_ED, LM_DF_test_ES], axis=1)\n",
    "LM_test_tot = pd.concat([LM_test, med_info_test], axis=1)\n",
    "\n",
    "class testing_model:\n",
    "    def __init__(self, data1, data2, data3, data4):\n",
    "        self.train1 = data1\n",
    "        self.train2 = data2\n",
    "        self.test1 = data3\n",
    "        self.test2 = data4\n",
    "        \n",
    "    def concatenate(self):\n",
    "        X_train = pd.concat([self.train1, self.train2], axis=1)\n",
    "        X_test = pd.concat([self.test1, self.test2], axis = 1)\n",
    "        \n",
    "        return X_train, X_test\n",
    "\n",
    "def processing(X_train, y_train, X_test, y_test):\n",
    "    # tools scaling and labelling\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_test = encoder.fit_transform(y_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# ### 3. Machine Learning\n",
    "\n",
    "# - prueba 1 = Radiomics + ED\n",
    "# - prueba 2 = Radiomics + ES\n",
    "# - prueba 3 = Radiomics + ES + ED\n",
    "# - prubea 4 = Radiomics + ES + ED + Med\n",
    "\n",
    "# In[112]:\n",
    "\n",
    "\n",
    "# In[113]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### Grid Search K-Best\n",
    "\n",
    "# In[156]:\n",
    "\n",
    "\n",
    "def KBest_GS(X_train, y_train, X_test, y_test, model, param_grid, df):\n",
    "    \n",
    "    featss =np.arange(3,X_train.shape[1])\n",
    "    \n",
    "    selector = SelectKBest()\n",
    "    \n",
    "    ### Pipeline\n",
    "    \n",
    "    ### we would need to adapt the \"NUMBER OF FEATURES PARAMETER OF THE GRID\"\n",
    "    \n",
    "    pipe = Pipeline([('selector', selector), \n",
    "                 ('model', model)])\n",
    "    \n",
    "    dict_1 = {'selector__score_func': [f_classif, chi2],\n",
    "              'selector__k':featss}   #### para pruebas\n",
    "    \n",
    "    dict_1.update(param_grid)\n",
    "    \n",
    "    gs = GridSearchCV(estimator=pipe, \n",
    "                  param_grid=dict_1, \n",
    "                  scoring='accuracy', \n",
    "                  n_jobs=1, \n",
    "                  cv=StratifiedKFold(4, shuffle=True, random_state=42),\n",
    "                  iid=True,\n",
    "                  refit=True,\n",
    "                verbose=3)\n",
    "    \n",
    "    print(pipe.get_params().keys())\n",
    "    \n",
    "    gs = gs.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best Model\", gs.best_params_)\n",
    "    \n",
    "    print('Best score:', gs.best_score_)\n",
    "    \n",
    "    y_test_pred = gs.predict(X_test)\n",
    "    \n",
    "    test_acc = accuracy_score(y_test,y_test_pred)\n",
    "    \n",
    "    print(\"\\n Test Accuracy with best estimator: \", test_acc)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(cm)\n",
    "        \n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(8,4))\n",
    "    \n",
    "    plt.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    sns.heatmap(cm, cmap=plt.cm.Blues, annot=True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    class_list = ['DCM', 'HCM', 'MINF', 'NOR', 'RV']\n",
    "\n",
    "    tick_marks = np.arange(len(class_list))\n",
    "    plt.xticks(tick_marks+0.5, class_list, rotation=45)\n",
    "    plt.yticks(tick_marks+0.5, class_list)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_test, y_test_pred,target_names=class_list))\n",
    "    \n",
    "    cols = gs.best_estimator_.steps[0][1].get_support(indices=True)\n",
    "    print(df.iloc[:,cols].columns)\n",
    "    K_best= df.iloc[:,cols].columns\n",
    "    \n",
    "    \n",
    "    return gs, K_best\n",
    "    \n",
    "\n",
    "\n",
    "# --- \n",
    "# #### Grid Search Sequential Forward Elimination\n",
    "\n",
    "def SFS_GS(X_train, y_train, X_test, y_test, model, param_grid, df):\n",
    "    # Setting up the SFS\n",
    "    sfs1 = SFS(estimator=model,\n",
    "               k_features=15,\n",
    "               forward=True,\n",
    "               floating=False,\n",
    "               scoring='accuracy',\n",
    "               cv=StratifiedKFold(3, shuffle=True, random_state=42))\n",
    "\n",
    "    ### Pipeline\n",
    "\n",
    "    ### we would need to adapt the \"NUMBER OF FEATURES PARAMETER OF THE GRID\"\n",
    "\n",
    "    pipe = Pipeline([('sfs', sfs1),\n",
    "                     ('model', model)])\n",
    "\n",
    "    # dict_1 = {'sfs__k_features':list(range(1,X_train.shape[1]))}   #### para pruebas\n",
    "\n",
    "    dict_1 = {'sfs__k_features': [5,10]}  # pruebas\n",
    "\n",
    "    dict_1.update(param_grid)\n",
    "\n",
    "    gs = GridSearchCV(estimator=pipe,\n",
    "                      param_grid=dict_1,\n",
    "                      scoring='accuracy',\n",
    "                      n_jobs=1,\n",
    "                      cv=StratifiedKFold(4, shuffle=True, random_state=42),\n",
    "                      verbose=3,\n",
    "                      refit=True)\n",
    "\n",
    "    #     print(pipe.get_params().keys())\n",
    "\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "\n",
    "    #     print(gs.best_estimator_.steps)\n",
    "\n",
    "    print(\"Best Model\", gs.best_params_)\n",
    "\n",
    "    #     print('Best score:', gs.best_score_)\n",
    "\n",
    "    y_test_pred = gs.predict(X_test)\n",
    "\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    print(\"\\n Test Accuracy with best estimator: \", test_acc)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    plt.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    sns.heatmap(cm, cmap=plt.cm.Blues, annot=True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    class_list = ['DCM', 'HCM', 'MINF', 'NOR', 'RV']\n",
    "\n",
    "    tick_marks = np.arange(len(class_list))\n",
    "    plt.xticks(tick_marks+0.5, class_list, rotation=45)\n",
    "    plt.yticks(tick_marks+0.5, class_list)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_test, y_test_pred, target_names=class_list))\n",
    "\n",
    "    feats = gs.best_estimator_.steps[0][1].k_feature_idx_\n",
    "\n",
    "    feats_2 = np.asanyarray(feats)\n",
    "\n",
    "    print(df.iloc[:, feats_2].columns)\n",
    "    feats_names = df.iloc[:, feats_2].columns\n",
    "\n",
    "    return gs, pipe, feats_names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[255]:\n",
    "\n",
    "\n",
    "#Support Vector Classifier\n",
    "\n",
    "model_SVC = SVC(kernel = 'linear', gamma = 'scale', max_iter= 1000, random_state=42)\n",
    "model_SVC_2 = SVC(kernel = 'linear', gamma = 'scale', max_iter= 5000, random_state=42)\n",
    "\n",
    "param_grid_SVC =  {'model__kernel':['linear'],\n",
    "                   'model__C':[5, 10, 12]}\n",
    "\n",
    "param_grid_SVC_nested_2 =  { 'selector__estimator__kernel': ['linear', 'rbf'],\n",
    "                   'selector__estimator__C':[15]}\n",
    "\n",
    "param_grid_SVC_test_2 =  { \n",
    "                   'estimator__model__C':[0.5, 1,5,10]}\n",
    "\n",
    "\n",
    "# In[82]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[83]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "\n",
    "# --- \n",
    "# #### Using the features extracted from the Inception\n",
    "\n",
    "# In[226]:\n",
    "\n",
    "#\n",
    "# print('Incep Features')\n",
    "# X_train1, X_test1 = testing_model(radiomics_train, incep_DF_train_ED, radiomics_test, incep_DF_test_ED).concatenate()\n",
    "# X_train2, X_test2 = testing_model(radiomics_train, incep_DF_train_ES, radiomics_test, incep_DF_test_ES).concatenate()\n",
    "# X_train3, X_test3 = testing_model(radiomics_train, incep_DF_train, radiomics_test, incep_DF_test).concatenate()\n",
    "# X_train4, X_test4 = testing_model(radiomics_train, incep_train_tot, radiomics_test, incep_test_tot).concatenate()\n",
    "#\n",
    "# X_train_1, y_train, X_test_1, y_test= processing(X_train1, y_train, X_test1, y_test)\n",
    "#\n",
    "# X_train_2, y_train, X_test_2, y_test= processing(X_train2, y_train, X_test2, y_test)\n",
    "#\n",
    "# X_train_3, y_train, X_test_3, y_test= processing(X_train3, y_train, X_test3, y_test)\n",
    "#\n",
    "# X_train_4, y_train, X_test_4, y_test= processing(X_train4, y_train, X_test4, y_test)\n",
    "# #\n",
    "#\n",
    "# gs, K_best = KBest_GS(X_train_3, y_train, X_test_3, y_test, model_SVC, param_grid_SVC, X_train3)\n",
    "\n",
    "\n",
    "\n",
    "# --- \n",
    "# #### Using the features extracted from the AlexNet\n",
    "\n",
    "\n",
    "\n",
    "X_train1, X_test1 = testing_model(radiomics_train, AlexNet_DF_train_ED, radiomics_test, AlexNet_DF_test_ED).concatenate()\n",
    "X_train2, X_test2 = testing_model(radiomics_train, AlexNet_DF_train_ES, radiomics_test, AlexNet_DF_test_ES).concatenate()\n",
    "X_train3, X_test3 = testing_model(radiomics_train, AlexNet_DF_train, radiomics_test, AlexNet_DF_test).concatenate()\n",
    "X_train4, X_test4 = testing_model(radiomics_train, AlexNet_train_tot, radiomics_test, AlexNet_test_tot).concatenate()\n",
    "\n",
    "\n",
    "X_train_1, y_train, X_test_1, y_test= processing(X_train1, y_train, X_test1, y_test)\n",
    "\n",
    "X_train_2, y_train, X_test_2, y_test= processing(X_train2, y_train, X_test2, y_test)\n",
    "\n",
    "X_train_3, y_train, X_test_3, y_test= processing(X_train3, y_train, X_test3, y_test)\n",
    "\n",
    "X_train_4, y_train, X_test_4, y_test= processing(X_train4, y_train, X_test4, y_test)\n",
    "\n",
    "# gs, K_best= KBest_GS(X_train_4, y_train, X_test_4, y_test, model_SVC, param_grid_SVC, X_train4)\n",
    "\n",
    "\n",
    "# # In[249]:\n",
    "# print('LM Features')\n",
    "# X_train1, X_test1 = testing_model(radiomics_train, LM_DF_train_ED, radiomics_test, LM_DF_test_ED).concatenate()\n",
    "# X_train2, X_test2 = testing_model(radiomics_train, LM_DF_train_ES, radiomics_test, LM_DF_test_ES).concatenate()\n",
    "# X_train3, X_test3 = testing_model(radiomics_train, LM_DF_train, radiomics_test, LM_test).concatenate()\n",
    "# X_train4, X_test4 = testing_model(radiomics_train, LM_train_tot, radiomics_test, LM_test_tot).concatenate()\n",
    "#\n",
    "# X_train_1, y_train, X_test_1, y_test= processing(X_train1, y_train, X_test1, y_test)\n",
    "#\n",
    "# X_train_2, y_train, X_test_2, y_test= processing(X_train2, y_train, X_test2, y_test)\n",
    "#\n",
    "# X_train_3, y_train, X_test_3, y_test= processing(X_train3, y_train, X_test3, y_test)\n",
    "#\n",
    "# X_train_4, y_train, X_test_4, y_test= processing(X_train4, y_train, X_test4, y_test)\n",
    "\n",
    "# gs, K_best = KBest_GS(X_train_3, y_train, X_test_3, y_test, model_SVC, param_grid_SVC, X_train3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[260]:\n",
    "\n",
    "\n",
    "#gs, pipe = SFS_GS(X_train_2, y_train, X_test_2, y_test, model_SVC, param_grid_SVC, X_train2)\n",
    "\n",
    "\n",
    "# In[261]:\n",
    "\n",
    "\n",
    "#gs, pipe = SFS_GS(X_train_4, y_train, X_test_4, y_test, model_SVC, param_grid_SVC, X_train4)\n",
    "\n",
    "\n",
    "# In[165]:\n",
    "\n",
    "\n",
    "#gs = KBest_GS(X_train_2, y_train, X_test_2, y_test, model_SVC, param_grid_SVC, X_train2)\n",
    "\n",
    "\n",
    "# # ---\n",
    "# # #### Using the features extracted from the VGG\n",
    "#\n",
    "# # In[324]:\n",
    "#\n",
    "#\n",
    "# X_train_df_3 = pd.concat([radiomics_train, VGG_DF_train], axis=1)\n",
    "# X_test_df_3 = pd.concat([radiomics_test, VGG_DF_test], axis=1)\n",
    "\n",
    "\n",
    "# In[325]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- \n",
    "# #### Using the features extracted from the AlexNetDil\n",
    "\n",
    "# In[175]:\n",
    "\n",
    "\n",
    "# X_train1, X_test1 = testing_model(radiomics_train, AlexNetdil_DF_train_ED, radiomics_test, AlexNetdil_DF_test_ED).concatenate()\n",
    "# X_train2, X_test2 = testing_model(radiomics_train, AlexNetdil_DF_train_ES, radiomics_test, AlexNetdil_DF_test_ES).concatenate()\n",
    "# X_train3, X_test3 = testing_model(radiomics_train, AlexNetdil_DF_train, radiomics_test, AlexNetdil_DF_test).concatenate()\n",
    "# X_train4, X_test4 = testing_model(radiomics_train, AlexNetdil_DF_train_tot, radiomics_test, AlexNetdil_DF_test_tot).concatenate()\n",
    "\n",
    "\n",
    "# In[215]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- \n",
    "# #### Using the features extracted from the Late Merging Model\n",
    "\n",
    "# In[262]:\n",
    "\n",
    "\n",
    "# X_train1, X_test1 = testing_model(radiomics_train, LM_train_ED, radiomics_test, LM_DF_test_ED).concatenate()\n",
    "# X_train2, X_test2 = testing_model(radiomics_train, LM_train_ES, radiomics_test, LM_DF_test_ES).concatenate()\n",
    "# X_train3, X_test3 = testing_model(radiomics_train, LM_train, radiomics_test, LM_test).concatenate()\n",
    "# X_train4, X_test4 = testing_model(radiomics_train, LM_train_tot, radiomics_test, LM_test_tot).concatenate()\n",
    "#\n",
    "# X_train_1, y_train, X_test_1, y_test= processing(X_train1, y_train, X_test1, y_test)\n",
    "#\n",
    "# X_train_2, y_train, X_test_2, y_test= processing(X_train2, y_train, X_test2, y_test)\n",
    "#\n",
    "# X_train_3, y_train, X_test_3, y_test= processing(X_train3, y_train, X_test3, y_test)\n",
    "#\n",
    "# X_train_4, y_train, X_test_4, y_test= processing(X_train4, y_train, X_test4, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#gs = KBest_GS(X_train_2, y_train, X_test_2, y_test, model_SVC, param_grid_SVC, X_train2)\n",
    "\n",
    "\n",
    "# In[268]:\n",
    "\n",
    "\n",
    "#gs, pipe = SFS_GS(X_train_4, y_train, X_test_4, y_test, model_SVC, param_grid_SVC, X_train4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
